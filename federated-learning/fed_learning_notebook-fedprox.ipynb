{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ir739wb/ilyarekun/bc_project/federated-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flwr as fl                    # Flower framework for federated learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import kagglehub                     # Utility to download datasets from Kaggle\n",
    "import shutil                        # For file operations like moving or copying files\n",
    "from sklearn.metrics import precision_recall_fscore_support  # For computing classification metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from flwr.common import parameters_to_ndarrays  # Convert model parameters to NumPy arrays for Flower\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set seeds for reproducibility across CPU, GPU, and NumPy\n",
    "# --------------------------------------------------------------------------------\n",
    "seed = 42\n",
    "torch.manual_seed(seed)                # Seed PyTorch CPU random number generator\n",
    "torch.cuda.manual_seed(seed)           # Seed the current GPU’s random number generator\n",
    "torch.cuda.manual_seed_all(seed)       # Seed all available GPUs\n",
    "np.random.seed(seed)                   # Seed NumPy random number generator\n",
    "# Ensure deterministic behavior in cuDNN (may reduce performance but ensures reproducible results)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Define computation device: Use GPU if available, otherwise CPU\n",
    "# --------------------------------------------------------------------------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.45),\n",
    "            nn.Conv2d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(256, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(512, 4),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)  # Flatten\n",
    "        out = self.fc_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping stops training when the validation loss has not improved after a given patience\n",
    "    or when it falls below a specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=0, threshold=0.19):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many calls with no improvement to wait before stopping.\n",
    "            delta (float): Minimum change in the monitored value to qualify as an improvement.\n",
    "            threshold (float): Absolute validation loss threshold; stop immediately if reached or below.\n",
    "        \"\"\"\n",
    "        self.patience = patience              # Number of rounds to wait for an improvement\n",
    "        self.delta = delta                    # Minimum decrease in loss to count as improvement\n",
    "        self.threshold = threshold            # Immediate stop if validation loss <= threshold\n",
    "        self.best_score = None                # Best score seen so far (negative val_loss)\n",
    "        self.early_stop = False               # Flag indicating whether to stop training\n",
    "        self.counter = 0                      # Counter for rounds without improvement\n",
    "        self.best_model_state = None          # State dictionary of the best model encountered\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on validation loss.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current validation loss.\n",
    "            model (nn.Module): Model to save if it is the best so far.\n",
    "        \"\"\"\n",
    "        # If validation loss is below or equal to the threshold, stop immediately\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"Val loss {val_loss:.5f} is below threshold {self.threshold}. Stopping training.\")\n",
    "            self.early_stop = True\n",
    "            # Save the current model state as the best\n",
    "            self.best_model_state = model.state_dict()\n",
    "            return\n",
    "\n",
    "        # Convert validation loss to a score we want to maximize (negative loss)\n",
    "        score = -val_loss\n",
    "\n",
    "        # If this is the first call, record current score and model state\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        # If no sufficient improvement, increment counter\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            # If patience is exceeded, set early_stop flag\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Improvement found: update best score and reset counter\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Load the saved best model state into the provided model.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Model instance to load the best state into.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_tumor_IID(num_clients=4):\n",
    "    \"\"\"\n",
    "    Download the brain tumor MRI dataset from Kaggle, merge training and testing folders\n",
    "    into a single directory, perform a stratified train/val/test split, and then\n",
    "    partition the training data evenly (IID) among a specified number of clients.\n",
    "    \n",
    "    Args:\n",
    "        num_clients (int): Number of clients to split the training set into. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "        client_train_loaders (list of DataLoader): List of DataLoader objects, one per client.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "    \"\"\"\n",
    "    # Download the dataset from Kaggle and get the local directory path\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    \n",
    "    # Define paths to the original \"Training\" and \"Testing\" subdirectories\n",
    "    train_path = os.path.join(dataset_path, \"Training\")\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")\n",
    "    \n",
    "    # Create a new directory to hold all images together (General_Dataset)\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)\n",
    "    \n",
    "    # Move images from both Training and Testing into General_Dataset, preserving class subfolders\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))\n",
    "    \n",
    "    # Define image preprocessing steps: center-crop, resize, convert to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),  # Crop center 400x400 region\n",
    "        transforms.Resize((200, 200)),      # Resize to 200x200 pixels\n",
    "        transforms.ToTensor(),              # Convert PIL image to PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Load all images from General_Dataset using ImageFolder (expects subdirectories as class labels)\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets             # List of numeric class labels for each image\n",
    "    classes = list(set(targets))                  # Unique class labels in the dataset\n",
    "    \n",
    "    # Initialize lists to hold indices for train/validation/test splits\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Define split ratios for each class\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "    \n",
    "    # Perform stratified splitting by class\n",
    "    for class_label in classes:\n",
    "        # Collect all indices whose label matches the current class\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        \n",
    "        # Calculate number of samples in each split for this class\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        test_size = class_size - train_size - val_size  # Remainder goes to test\n",
    "        \n",
    "        # Assign the first train_size indices to the training set\n",
    "        train_indices.extend(class_indices[:train_size])\n",
    "        # Assign the next val_size indices to the validation set\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])\n",
    "        # Assign the remaining indices to the test set\n",
    "        test_indices.extend(class_indices[train_size + val_size:])\n",
    "    \n",
    "    # Create Subset objects based on computed indices for train, val, test\n",
    "    train_set = Subset(general_dataset, train_indices)\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    \n",
    "    # Create DataLoaders for validation and test sets (no shuffling)\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Prepare to split the training indices evenly (IID) among num_clients\n",
    "    client_indices = {client: [] for client in range(num_clients)}\n",
    "    for class_label in classes:\n",
    "        # Filter training indices that belong to the current class\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)  # Shuffle to randomize assignment\n",
    "        \n",
    "        # Split this class's training indices into num_clients parts\n",
    "        splits = np.array_split(class_train_indices, num_clients)\n",
    "        # Assign each split part to the corresponding client's index list\n",
    "        for client in range(num_clients):\n",
    "            client_indices[client].extend(splits[client].tolist())\n",
    "    \n",
    "    # Create a DataLoader for each client's training subset\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    # Return the list of client-specific train loaders, and shared val/test loaders\n",
    "    return client_train_loaders, val_loader, test_loader\n",
    "\n",
    "# Compute data loaders for 4 clients\n",
    "#client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_IID(num_clients=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_tumor_NON_IID(num_clients=4):\n",
    "    \"\"\"\n",
    "    Download the brain tumor MRI dataset from Kaggle, merge training and testing data\n",
    "    into a single directory, perform a stratified train/validation/test split, and\n",
    "    partition the training data in a non-IID manner across multiple clients.\n",
    "    \n",
    "    Args:\n",
    "        num_clients (int): Number of clients to split the training set into. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "        client_train_loaders (list of DataLoader): List of DataLoader objects, one per client.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------\n",
    "    # Download and merge the dataset\n",
    "    # ----------------------------------------\n",
    "    # Download dataset from Kaggle and get local path\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    # Define the original training and testing subdirectories\n",
    "    train_path = os.path.join(dataset_path, \"Training\")\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")\n",
    "    # Create a new directory for holding all images together\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)\n",
    "    \n",
    "    # Move all images from \"Training\" and \"Testing\" into \"General_Dataset\"\n",
    "    # so that ImageFolder can load them uniformly by class subfolders\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Define image transformations\n",
    "    # ----------------------------------------\n",
    "    # Center-crop to 400x400, then resize to 200x200, and convert to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),  # Crop center region of size 400x400\n",
    "        transforms.Resize((200, 200)),      # Resize cropped image to 200x200\n",
    "        transforms.ToTensor(),              # Convert PIL image to PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Load the combined dataset with ImageFolder\n",
    "    # ----------------------------------------\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets         # List of numeric class labels for each image\n",
    "    classes = list(set(targets))              # Unique class labels present in the dataset\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Stratified train/validation/test split\n",
    "    # ----------------------------------------\n",
    "    train_indices = []   # Will hold indices for training samples\n",
    "    val_indices = []     # Will hold indices for validation samples\n",
    "    test_indices = []    # Will hold indices for test samples\n",
    "    \n",
    "    # Define split ratios: 70% train, 20% val, remainder (10%) test\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    # test_ratio will implicitly be 0.1\n",
    "    \n",
    "    # For each class, split its indices into train/val/test\n",
    "    for class_label in classes:\n",
    "        # Collect all indices belonging to this class\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        # Compute how many samples go to train and validation for this class\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        # Assign the first train_size indices to training\n",
    "        train_indices.extend(class_indices[:train_size])\n",
    "        # Assign the next val_size indices to validation\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])\n",
    "        # Assign the remaining indices to test\n",
    "        test_indices.extend(class_indices[train_size + val_size:])\n",
    "    \n",
    "    # Create Subset objects for train, val, and test sets\n",
    "    train_set = Subset(general_dataset, train_indices)\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    \n",
    "    # Create DataLoaders for validation and test sets\n",
    "    # (no shuffling needed for val/test)\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Non-IID partitioning of training data\n",
    "    # ----------------------------------------\n",
    "    # Define a fixed distribution for 4 classes across 4 clients\n",
    "    # Each list is the percentage of that class assigned to each client\n",
    "    distribution = {\n",
    "        0: [0.70, 0.15, 0.10, 0.05],  # Class 0: client0=70%, client1=15%, client2=10%, client3=5%\n",
    "        1: [0.15, 0.70, 0.10, 0.05],  # Class 1: client0=15%, client1=70%, client2=10%, client3=5%\n",
    "        2: [0.10, 0.15, 0.70, 0.05],  # Class 2: client0=10%, client1=15%, client2=70%, client3=5%\n",
    "        3: [0.05, 0.10, 0.15, 0.70]   # Class 3: client0=5%, client1=10%, client2=15%, client3=70%\n",
    "    }\n",
    "    \n",
    "    # Initialize a dictionary to collect training indices per client\n",
    "    client_indices = {client: [] for client in range(num_clients)}\n",
    "    \n",
    "    # For each class, assign training samples to clients according to distribution\n",
    "    for class_label in classes:\n",
    "        # Extract training indices for this class\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)  # Shuffle indices before splitting\n",
    "        \n",
    "        n = len(class_train_indices)  # Total number of training samples for this class\n",
    "        allocation = []\n",
    "        # Calculate the number of samples per client for this class\n",
    "        for client in range(num_clients):\n",
    "            cnt = int(distribution[class_label][client] * n)\n",
    "            allocation.append(cnt)\n",
    "        # Adjust the last client's allocation to ensure the sum matches n\n",
    "        allocation[-1] = n - sum(allocation[:-1])\n",
    "        \n",
    "        # Distribute the shuffled indices according to the calculated allocation\n",
    "        start = 0\n",
    "        for client in range(num_clients):\n",
    "            cnt = allocation[client]\n",
    "            # Assign a slice of class_train_indices to this client\n",
    "            client_indices[client].extend(class_train_indices[start:start + cnt])\n",
    "            start += cnt\n",
    "    \n",
    "    # Create a DataLoader for each client's training partition\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    # Return the list of client-specific train loaders, plus shared val_loader and test_loader\n",
    "    return client_train_loaders, val_loader, test_loader\n",
    "\n",
    "# Compute data loaders for 4 clients\n",
    "client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_NON_IID(num_clients=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Instantiate a new BrainCNN model and move it to the configured device (CPU or GPU).\n",
    "    \"\"\"\n",
    "    return BrainCNN().to(device)\n",
    "\n",
    "\n",
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Create and return an optimizer for the provided model.\n",
    "    \n",
    "    Notes on experiment history (do not modify these commented lines):\n",
    "        _: lr = 0.001, weight_decay=0.001\n",
    "        #return optim.SGD(model.parameters(), lr=0.0008, momentum=0.7, weight_decay=0.09)  # 1 iid final\n",
    "    The active configuration below was chosen for the current experiments:\n",
    "    \"\"\"\n",
    "    return optim.SGD(model.parameters(), lr=0.001, momentum=0.7, weight_decay=0.09)\n",
    "\n",
    "\n",
    "def get_loss_function():\n",
    "    \"\"\"\n",
    "    Return the loss function to be used during training and evaluation.\n",
    "    Using CrossEntropyLoss for multi-class classification.\n",
    "    \"\"\"\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def fit_config(server_round: int):\n",
    "    \"\"\"\n",
    "    Return the training configuration for each federated round.\n",
    "    \n",
    "    Args:\n",
    "        server_round (int): Current federated learning round (not used here).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the number of local epochs each client should perform.\n",
    "    \"\"\"\n",
    "    return {\"local_epochs\": 5}  # 5 local epochs per client per round\n",
    "\n",
    "\n",
    "def evaluate_fn(server_round, parameters, config):\n",
    "    \"\"\"\n",
    "    Evaluate the global model on the validation set after receiving updated parameters.\n",
    "    \n",
    "    Args:\n",
    "        server_round (int): Current federated learning round (unused here).\n",
    "        parameters (list of np.ndarray): Model weights from the server to load into a fresh model.\n",
    "        config (dict): Additional configuration (unused here).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - val_loss (float): The average loss on the validation dataset.\n",
    "            - metrics (dict): Dictionary with keys \"accuracy\", \"precision\", \"recall\", \"f1\".\n",
    "    \n",
    "    Procedure:\n",
    "        1. Recreate a new model and load the received parameters into it.\n",
    "        2. Set the model to evaluation mode.\n",
    "        3. Iterate through the global val_loader (assumed defined globally), accumulate loss and predictions.\n",
    "        4. Compute average loss and classification metrics.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate a fresh model and move it to the correct device\n",
    "    model = get_model()\n",
    "    # Map the list of weight arrays back to the model’s state_dict\n",
    "    state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), parameters)}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # Switch to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "    # 2. Set up loss function\n",
    "    criterion = get_loss_function()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # 3. Iterate over validation batches (assumes val_loader is defined globally)\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # Accumulate weighted loss by batch size\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Get predicted class indices\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # 4. Compute average validation loss across all samples\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    # Compute accuracy, precision, recall, and F1 (macro-averaged)\n",
    "    accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Return the computed validation loss and metrics dictionary\n",
    "    return val_loss, {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import Context\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\"\n",
    "    Factory function to create a Flower client instance based on the provided context.\n",
    "    Args:\n",
    "        context (Context): Flower context containing node_id and configuration.\n",
    "    Returns:\n",
    "        flower.client.NumPyClient: A configured Flower client for federated training.\n",
    "    \"\"\"\n",
    "    # Determine which client index to use based on the node ID provided by Flower.\n",
    "    # We mod by the number of client_train_loaders to cycle through them.\n",
    "    # Alternative approaches (commented out) could extract a custom cid from context.node_config.\n",
    "    # cid = context.node_config[\"cid\"]\n",
    "    # cid = context.node_id\n",
    "    cid = int(context.node_id) % len(client_train_loaders)\n",
    "\n",
    "    # Initialize a fresh BrainCNN model and move it to the configured device (CPU or GPU).\n",
    "    model = BrainCNN().to(device)\n",
    "    # Create an optimizer for the model using the chosen hyperparameters.\n",
    "    optimizer = get_optimizer(model)\n",
    "    # Define the loss function to use during training and evaluation.\n",
    "    criterion = get_loss_function()\n",
    "    # Select the local training DataLoader corresponding to this client.\n",
    "    train_loader = client_train_loaders[int(cid)]\n",
    "\n",
    "    class FlowerClient(fl.client.NumPyClient):\n",
    "        \"\"\"\n",
    "        Implementation of the Flower NumPyClient interface for federated learning.\n",
    "        Each method defines how the client exchanges parameters, trains locally (fit),\n",
    "        and performs local evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_parameters(self, config):\n",
    "            \"\"\"\n",
    "            Return the current local model parameters as a list of NumPy arrays.\n",
    "            Flower will gather these to send to the server.\n",
    "            Args:\n",
    "                config (dict): Configuration dictionary provided by the server (unused here).\n",
    "            Returns:\n",
    "                List[np.ndarray]: Model weights converted to NumPy arrays.\n",
    "            \"\"\"\n",
    "            return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "        def fit(self, parameters, config):\n",
    "            \"\"\"\n",
    "            Perform local training on this client's data, optionally including a proximal term\n",
    "            for FedProx-style regularization.\n",
    "            Args:\n",
    "                parameters (List[np.ndarray]): Global model weights received from the server.\n",
    "                config (dict): Configuration dictionary containing \"local_epochs\" and possibly \"proximal_mu\".\n",
    "            Returns:\n",
    "                Tuple[List[np.ndarray], int, dict]:\n",
    "                    - Updated model parameters after local training\n",
    "                    - Number of training examples used\n",
    "                    - Metrics dictionary (empty here)\n",
    "            \"\"\"\n",
    "            # 1. Load the global parameters into the local model\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v) \n",
    "                for k, v in zip(model.state_dict().keys(), parameters)\n",
    "            }\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.train()  # Switch model to training mode\n",
    "\n",
    "            # 2. Read proximal_mu from config to control FedProx penalty term\n",
    "            proximal_mu = config.get(\"proximal_mu\", 0.0)\n",
    "\n",
    "            # 3. If using FedProx (proximal_mu > 0), save a copy of global parameters\n",
    "            if proximal_mu > 0:\n",
    "                # Clone and detach global parameters to use as reference during local updates\n",
    "                global_params = [p.clone().detach() for p in model.parameters()]\n",
    "            else:\n",
    "                global_params = None\n",
    "\n",
    "            # 4. Training loop: run the specified number of local epochs\n",
    "            for _ in range(config[\"local_epochs\"]):\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()       # Zero out gradients before each batch\n",
    "                    output = model(data)        # Forward pass\n",
    "                    loss = criterion(output, target)  # Compute classification loss\n",
    "\n",
    "                    # 5. If FedProx is enabled, add the proximal term to the loss\n",
    "                    if proximal_mu > 0:\n",
    "                        proximal_term = 0.0\n",
    "                        # Sum squared differences between local and global parameters\n",
    "                        for local_param, global_param in zip(model.parameters(), global_params):\n",
    "                            proximal_term += (local_param - global_param).pow(2).sum()\n",
    "                        # Add the scaled proximal term to the original loss\n",
    "                        loss += (proximal_mu / 2) * proximal_term\n",
    "\n",
    "                    loss.backward()  # Backpropagate gradients\n",
    "                    optimizer.step()  # Update model parameters\n",
    "\n",
    "            # After training, return the updated weights and the number of examples used\n",
    "            return self.get_parameters(config), len(train_loader.dataset), {}\n",
    "\n",
    "        def evaluate(self, parameters, config):\n",
    "            \"\"\"\n",
    "            Perform local evaluation on this client's training set (used as proxy here).\n",
    "            Args:\n",
    "                parameters (List[np.ndarray]): Global model weights from the server.\n",
    "                config (dict): Configuration dictionary (unused for evaluation).\n",
    "            Returns:\n",
    "                Tuple[float, int, dict]:\n",
    "                    - Average loss over the local dataset\n",
    "                    - Number of examples used for evaluation\n",
    "                    - Metrics dictionary (accuracy placeholder here)\n",
    "            \"\"\"\n",
    "            # 1. Load the received global parameters into the model\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v) \n",
    "                for k, v in zip(model.state_dict().keys(), parameters)\n",
    "            }\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()  # Switch model to evaluation mode\n",
    "\n",
    "            loss = 0.0\n",
    "            num_examples = 0\n",
    "\n",
    "            # 2. Accumulate loss over all local data without computing gradients\n",
    "            with torch.no_grad():\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss += criterion(output, target).item()\n",
    "                    num_examples += data.size(0)  # Count examples processed\n",
    "\n",
    "            # 3. Compute average loss; avoid division by zero if loader is empty\n",
    "            avg_loss = loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "            # Placeholder metrics dictionary; replace with actual accuracy computation if desired\n",
    "            metrics = {\"accuracy\": 0.0}\n",
    "\n",
    "            return avg_loss, num_examples, metrics\n",
    "\n",
    "    # Instantiate and return the Flower client object for this federated node\n",
    "    return FlowerClient().to_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFedProx(fl.server.strategy.FedProx):\n",
    "    \"\"\"\n",
    "    Custom FedProx strategy that extends Flower's built-in FedProx by adding:\n",
    "    - Tracking of validation metrics history\n",
    "    - Early stopping based on validation loss threshold\n",
    "    \"\"\"\n",
    "    def __init__(self, proximal_mu=0.1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            proximal_mu (float): Coefficient for the FedProx proximal term.\n",
    "            *args, **kwargs: Additional arguments passed to the parent FedProx constructor.\n",
    "        \"\"\"\n",
    "        # Initialize the base FedProx strategy with the given proximal_mu and any other args\n",
    "        super().__init__(proximal_mu=proximal_mu, *args, **kwargs)\n",
    "        \n",
    "        # Initialize a dictionary to store metrics for each round:\n",
    "        # - Validation loss, accuracy, precision, recall, and F1 score\n",
    "        self.metrics_history = {\n",
    "            \"val_loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": []\n",
    "        }\n",
    "        \n",
    "        # Will hold the best or last model parameters depending on early stopping\n",
    "        self.final_parameters = None\n",
    "        \n",
    "        # Create an EarlyStopping instance with high patience and small delta for fine-grained stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=40,        # Number of rounds to wait without improvement\n",
    "            delta=0.00001,      # Minimum change in validation loss to count as improvement\n",
    "            threshold=0.0001    # Absolute threshold for validation loss to stop immediately\n",
    "        )\n",
    "    \n",
    "    def evaluate(self, server_round, parameters):\n",
    "        \"\"\"\n",
    "        Override the FedProx evaluate method to:\n",
    "        1. Call the parent evaluate to compute (val_loss, metrics) for the global model\n",
    "        2. Append those metrics to metrics_history\n",
    "        3. Load the global model with current parameters and run early stopping check\n",
    "        4. If early stopping is triggered, save best parameters and raise StopIteration\n",
    "        \"\"\"\n",
    "        # Call the base class evaluate, which uses evaluate_fn to compute loss & metrics\n",
    "        result = super().evaluate(server_round, parameters)\n",
    "        \n",
    "        # If evaluation returned valid results (i.e., validation set exists)\n",
    "        if result:\n",
    "            loss, metrics = result\n",
    "            \n",
    "            # Append the new validation loss and other metrics to history\n",
    "            self.metrics_history[\"val_loss\"].append(loss)\n",
    "            for key in metrics:\n",
    "                self.metrics_history[key].append(metrics[key])\n",
    "            \n",
    "            # Save the current parameters as the most recent parameters\n",
    "            self.final_parameters = parameters\n",
    "            \n",
    "            # Recreate a fresh global model and load the new parameters into it\n",
    "            global_model = get_model()  # get_model() returns a BrainCNN instance on the correct device\n",
    "            # Convert the Flower parameters to numpy arrays and build a state_dict\n",
    "            final_ndarrays = parameters_to_ndarrays(parameters)\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v) \n",
    "                for k, v in zip(global_model.state_dict().keys(), final_ndarrays)\n",
    "            }\n",
    "            global_model.load_state_dict(state_dict)\n",
    "            \n",
    "            # Check early stopping with the current validation loss and model\n",
    "            self.early_stopping(loss, global_model)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at round {server_round}.\")\n",
    "                \n",
    "                # Retrieve the best model state recorded by EarlyStopping\n",
    "                best_state_dict = self.early_stopping.best_model_state\n",
    "                # Convert that state_dict into a list of NumPy arrays in the same key order as the model\n",
    "                best_parameters = [\n",
    "                    best_state_dict[k].cpu().numpy() \n",
    "                    for k in global_model.state_dict().keys()\n",
    "                ]\n",
    "                # Update final_parameters to point to the best weights found so far\n",
    "                self.final_parameters = best_parameters\n",
    "                \n",
    "                # Raise StopIteration to instruct Flower to halt federated training early\n",
    "                raise StopIteration(\"Early stopping triggered.\")\n",
    "        \n",
    "        # Return the result (val_loss and metrics) if early stopping did not occur\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = CustomFedProx(\n",
    "    proximal_mu=0.9,  # Strength of the proximal term; adjust as needed\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=4,\n",
    "    min_available_clients=4,\n",
    "    evaluate_fn=evaluate_fn,\n",
    "    on_fit_config_fn=fit_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=4,\n",
    "        config=fl.server.ServerConfig(num_rounds=72),\n",
    "        strategy=strategy,\n",
    "        client_resources={\"num_cpus\": 2, \"num_gpus\": 0.5},\n",
    "        ray_init_args={\n",
    "            \"num_cpus\": 16,\n",
    "            \"object_store_memory\": 40 * 1024**3\n",
    "        }\n",
    "    )\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "print(\"Federated learning simulation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of rounds for plotting, starting from 1 to the length of the accuracy history\n",
    "rounds = range(1, len(strategy.metrics_history['accuracy']) + 1)\n",
    "\n",
    "# Create a new figure with a specified size (12 inches wide, 8 inches tall)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot validation loss over rounds, using the metrics stored in strategy.metrics_history\n",
    "plt.plot(rounds, strategy.metrics_history['val_loss'], label='Validation Loss')\n",
    "\n",
    "# Plot accuracy over rounds, using the metrics stored in strategy.metrics_history\n",
    "plt.plot(rounds, strategy.metrics_history['accuracy'], label='Accuracy')\n",
    "\n",
    "# Plot precision over rounds, using the metrics stored in strategy.metrics_history\n",
    "plt.plot(rounds, strategy.metrics_history['precision'], label='Precision')\n",
    "\n",
    "# Plot recall over rounds, using the metrics stored in strategy.metrics_history\n",
    "plt.plot(rounds, strategy.metrics_history['recall'], label='Recall')\n",
    "\n",
    "# Plot F1 score over rounds, using the metrics stored in strategy.metrics_history\n",
    "plt.plot(rounds, strategy.metrics_history['f1'], label='F1 Score')\n",
    "\n",
    "# Label the x-axis as 'Round' to indicate the federated learning rounds\n",
    "plt.xlabel('Round')\n",
    "\n",
    "# Label the y-axis as 'Metric Value' to represent the values of the plotted metrics\n",
    "plt.ylabel('Metric Value')\n",
    "\n",
    "# Set the title of the plot to describe the context of the metrics\n",
    "plt.title('Federated Learning Metrics Over Rounds')\n",
    "\n",
    "# Add a legend to distinguish between the different metric lines\n",
    "plt.legend()\n",
    "\n",
    "# Enable a grid in the plot for better readability of the metric values\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to a file with high resolution (300 DPI) before displaying it\n",
    "plt.savefig('/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fed-prox-non-iid-graph1.png', dpi=300)\n",
    "\n",
    "# Display the plot (useful in interactive environments like Jupyter notebooks)\n",
    "plt.show()\n",
    "\n",
    "# Close the figure to free memory and prevent overlap with future plots\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for federated learning, PyTorch, NumPy, and metrics computation\n",
    "from flwr.common import parameters_to_ndarrays  # Import function to convert Flower Parameters to NumPy arrays\n",
    "import torch  # Import PyTorch for model operations\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "from sklearn.metrics import precision_recall_fscore_support  # Import function to compute precision, recall, and F1 score\n",
    "\n",
    "# Check if early stopping was triggered to determine which model parameters to use\n",
    "if strategy.early_stopping.early_stop:\n",
    "    print(\"Using the best model parameters from early stopping.\")  # Inform user that early stopping parameters are used\n",
    "    best_parameters = strategy.final_parameters  # Use parameters saved by early stopping\n",
    "else:\n",
    "    print(\"Early stopping was not triggered. Using the final round's parameters.\")  # Inform user that final round parameters are used\n",
    "    best_parameters = strategy.final_parameters  # Use parameters from the last round\n",
    "\n",
    "# Create a new instance of the model (assumes get_model() returns BrainCNN().to(device))\n",
    "model = get_model()  # Instantiate the BrainCNN model and move it to the appropriate device (CPU/GPU)\n",
    "\n",
    "# Convert the federated learning parameters (Flower Parameters) to a list of NumPy arrays\n",
    "final_ndarrays = parameters_to_ndarrays(best_parameters)  # Transform parameters to NumPy format for compatibility\n",
    "\n",
    "# Create a state dictionary by mapping model parameter keys to corresponding NumPy arrays converted to PyTorch tensors\n",
    "state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), final_ndarrays)}\n",
    "\n",
    "# Load the state dictionary into the model to set its weights\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels for evaluation\n",
    "all_preds = []  # List to store predicted class labels\n",
    "all_targets = []  # List to store true class labels\n",
    "\n",
    "# Evaluate the model on the test set without computing gradients to save memory\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:  # Iterate over batches in the test data loader\n",
    "        data, target = data.to(device), target.to(device)  # Move data and targets to the appropriate device (CPU/GPU)\n",
    "        output = model(data)  # Perform a forward pass to get model predictions\n",
    "        _, predicted = torch.max(output, 1)  # Get the predicted class by selecting the index with the highest score\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Append predictions to the list (move to CPU and convert to NumPy)\n",
    "        all_targets.extend(target.cpu().numpy())  # Append true labels to the list (move to CPU and convert to NumPy)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = (np.array(all_preds) == np.array(all_targets)).mean()  # Calculate accuracy as the mean of correct predictions\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='macro', zero_division=0)  # Compute macro-averaged precision, recall, and F1 score\n",
    "\n",
    "# Print the test metrics with 4 decimal places for clarity\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")  # Display test set accuracy\n",
    "print(f\"Test Precision: {precision:.4f}\")  # Display test set precision\n",
    "print(f\"Test Recall: {recall:.4f}\")  # Display test set recall\n",
    "print(f\"Test F1 Score: {f1:.4f}\")  # Display test set F1 score\n",
    "\n",
    "# Define the file path to save the metrics\n",
    "metrics_file = '/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fed-prox-non-iid-metrics1.txt'\n",
    "\n",
    "# Save metrics to a text file, including per-round metrics and final test metrics\n",
    "with open(metrics_file, 'w') as f:\n",
    "    rounds = range(1, len(strategy.metrics_history['val_loss']) + 1)  # Define the range of rounds based on validation loss history\n",
    "    for round_num in rounds:  # Iterate over each round\n",
    "        f.write(f\"Round {round_num}:\\n\")  # Write the round number\n",
    "        f.write(f\"  Validation Loss: {strategy.metrics_history['val_loss'][round_num-1]:.4f}\\n\")  # Write validation loss for the round\n",
    "        f.write(f\"  Accuracy: {strategy.metrics_history['accuracy'][round_num-1]:.4f}\\n\")  # Write accuracy for the round\n",
    "        f.write(f\"  Precision: {strategy.metrics_history['precision'][round_num-1]:.4f}\\n\")  # Write precision for the round\n",
    "        f.write(f\"  Recall: {strategy.metrics_history['recall'][round_num-1]:.4f}\\n\")  # Write recall for the round\n",
    "        f.write(f\"  F1 Score: {strategy.metrics_history['f1'][round_num-1]:.4f}\\n\")  # Write F1 score for the round\n",
    "    f.write(\"\\nTest Metrics:\\n\")  # Add a header for final test metrics\n",
    "    f.write(f\"  Accuracy: {accuracy:.4f}\\n\")  # Write test set accuracy\n",
    "    f.write(f\"  Precision: {precision:.4f}\\n\")  # Write test set precision\n",
    "    f.write(f\"  Recall: {recall:.4f}\\n\")  # Write test set recall\n",
    "    f.write(f\"  F1 Score: {f1:.4f}\\n\")  # Write test set F1 score\n",
    "\n",
    "# Confirm that the metrics have been saved to the specified file\n",
    "print(f\"Metrics saved to '{metrics_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
