{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ir739wb/ilyarekun/bc_project/federated-learning/fed-avg-code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for file handling, federated learning, deep learning, and data processing\n",
    "import os  # For file and directory operations\n",
    "import flwr as fl  # Flower framework for federated learning\n",
    "import torch  # PyTorch for building and training neural networks\n",
    "import torch.nn as nn  # Neural network modules from PyTorch\n",
    "import torch.optim as optim  # Optimizers from PyTorch\n",
    "import torchvision.transforms as transforms  # Image transformations\n",
    "from torch.utils.data import DataLoader, Subset  # Data loading and subset utilities\n",
    "from torchvision.datasets import ImageFolder  # Dataset class for image folders\n",
    "import numpy as np  # Numerical computations\n",
    "import kagglehub  # For downloading datasets from Kaggle\n",
    "import shutil  # For file moving operations\n",
    "from sklearn.metrics import precision_recall_fscore_support  # Metrics for evaluation\n",
    "import matplotlib.pyplot as plt  # Plotting utilities\n",
    "from flwr.common import parameters_to_ndarrays  # Convert Flower parameters to NumPy arrays\n",
    "\n",
    "# Set seeds for reproducibility across random operations\n",
    "seed = 42\n",
    "torch.manual_seed(seed)  # Seed for PyTorch CPU\n",
    "torch.cuda.manual_seed(seed)  # Seed for PyTorch CUDA (single GPU)\n",
    "torch.cuda.manual_seed_all(seed)  # Seed for all CUDA devices\n",
    "np.random.seed(seed)  # Seed for NumPy\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in CuDNN\n",
    "torch.backends.cudnn.benchmark = False  # Disable benchmarking for reproducibility\n",
    "\n",
    "# Define the device for computation (GPU if available, else CPU)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")  # Inform the user about the selected device\n",
    "\n",
    "# Define the CNN model for brain tumor classification\n",
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainCNN, self).__init__()\n",
    "        # Convolutional layers with ReLU, batch normalization, max pooling, and dropout\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),  # Input: 3 channels (RGB), Output: 64 channels\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.BatchNorm2d(64),  # Normalize across 64 channels\n",
    "            nn.MaxPool2d(2),  # Downsample by 2x\n",
    "            nn.Dropout2d(0.45),  # Dropout to prevent overfitting\n",
    "            nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=3),  # Increase channels to 128\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3),  # Maintain 128 channels\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.45),\n",
    "            nn.Conv2d(128, 256, kernel_size=7, stride=1, padding=3),  # Increase to 256 channels\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(256, 256, kernel_size=7, stride=1, padding=3),  # Maintain 256 channels\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=1, padding=3),  # Increase to 512 channels\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "        # Fully connected layers for classification\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 1024),  # Flatten output (512 * 3 * 3 from conv layers) to 1024\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 512),  # Reduce to 512\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(512, 4),  # Output layer for 4 classes (tumor types)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)  # Pass input through convolutional layers\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output for fully connected layers\n",
    "        out = self.fc_layers(out)  # Pass through fully connected layers\n",
    "        return out\n",
    "\n",
    "# Class to implement early stopping based on validation loss\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, threshold=0.19):\n",
    "        self.patience = patience  # Number of rounds to wait for improvement\n",
    "        self.delta = delta  # Minimum change to qualify as improvement\n",
    "        self.threshold = threshold  # Loss threshold to stop training\n",
    "        self.best_score = None  # Best negative validation loss\n",
    "        self.early_stop = False  # Flag to indicate stopping\n",
    "        self.counter = 0  # Count rounds without improvement\n",
    "        self.best_model_state = None  # Store best model parameters\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        # Stop if validation loss is below threshold\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"Val loss {val_loss:.5f} ниже порогового значения {self.threshold}.\")  # Russian: \"below threshold\"\n",
    "            self.early_stop = True\n",
    "            self.best_model_state = model.state_dict()  # Save best model state\n",
    "            return\n",
    "\n",
    "        score = -val_loss  # Use negative loss for comparison (higher is better)\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score  # Initialize best score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1  # Increment counter if no improvement\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  # Trigger early stopping\n",
    "        else:\n",
    "            self.best_score = score  # Update best score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0  # Reset counter on improvement\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)  # Load best model parameters\n",
    "\n",
    "# Function to preprocess the brain tumor MRI dataset in a non-IID manner\n",
    "def data_preprocessing_tumor_NON_IID(num_clients=4):\n",
    "    # Download dataset from Kaggle\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    train_path = os.path.join(dataset_path, \"Training\")  # Training data path\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")  # Testing data path\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")  # Combined dataset path\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    # Combine training and testing data into a single dataset\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))  # Move images\n",
    "    \n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),  # Crop to 400x400\n",
    "        transforms.Resize((200, 200)),  # Resize to 200x200\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "    ])\n",
    "    \n",
    "    # Load the combined dataset\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets  # Class labels\n",
    "    classes = list(set(targets))  # Unique classes\n",
    "    \n",
    "    # Split dataset into train, validation, and test sets\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    train_ratio = 0.7  # 70% for training\n",
    "    val_ratio = 0.2  # 20% for validation (remaining 10% for testing)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        train_indices.extend(class_indices[:train_size])  # Training indices\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])  # Validation indices\n",
    "        test_indices.extend(class_indices[train_size + val_size:])  # Test indices\n",
    "    \n",
    "    # Create subsets for train, validation, and test\n",
    "    train_set = Subset(general_dataset, train_indices)\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    \n",
    "    # Create data loaders for validation and test sets\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Define non-IID distribution for clients (each client has a different class proportion)\n",
    "    distribution = {\n",
    "        0: [0.70, 0.15, 0.10, 0.05],  # Client 0: mostly class 0\n",
    "        1: [0.15, 0.70, 0.10, 0.05],  # Client 1: mostly class 1\n",
    "        2: [0.10, 0.15, 0.70, 0.05],  # Client 2: mostly class 2\n",
    "        3: [0.05, 0.10, 0.15, 0.70]   # Client 3: mostly class 3\n",
    "    }\n",
    "    \n",
    "    client_indices = {client: [] for client in range(num_clients)}  # Indices for each client\n",
    "    \n",
    "    # Distribute training data to clients in a non-IID manner\n",
    "    for class_label in classes:\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)  # Shuffle indices\n",
    "        \n",
    "        n = len(class_train_indices)  # Total samples for this class\n",
    "        allocation = []\n",
    "        for client in range(num_clients):\n",
    "            cnt = int(distribution[class_label][client] * n)  # Allocate based on distribution\n",
    "            allocation.append(cnt)\n",
    "        allocation[-1] = n - sum(allocation[:-1])  # Adjust last client to use all remaining samples\n",
    "        \n",
    "        start = 0\n",
    "        for client in range(num_clients):\n",
    "            cnt = allocation[client]\n",
    "            client_indices[client].extend(class_train_indices[start:start + cnt])  # Assign indices\n",
    "            start += cnt\n",
    "    \n",
    "    # Create data loaders for each client\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)  # Shuffle for training\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    return client_train_loaders, val_loader, test_loader  # Return all loaders\n",
    "\n",
    "# Preprocess data and get loaders for 4 clients\n",
    "client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_NON_IID(num_clients=4)\n",
    "\n",
    "# Helper function to instantiate the model\n",
    "def get_model():\n",
    "    return BrainCNN().to(device)  # Create model and move to device\n",
    "\n",
    "# Helper function to get the optimizer\n",
    "def get_optimizer(model):\n",
    "    return optim.SGD(model.parameters(), lr=0.001, momentum=0.7, weight_decay=0.09)  # SGD with momentum and weight decay\n",
    "\n",
    "# Helper function to get the loss function\n",
    "def get_loss_function():\n",
    "    return nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "\n",
    "# Configuration function for client training\n",
    "def fit_config(server_round: int):\n",
    "    return {\"local_epochs\": 5}  # Each client trains for 5 local epochs per round\n",
    "\n",
    "# Evaluation function for the server to assess the global model\n",
    "def evaluate_fn(server_round, parameters, config):\n",
    "    model = get_model()  # Create a new model instance\n",
    "    state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), parameters)}  # Load parameters\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    criterion = get_loss_function()  # Get loss function\n",
    "    val_loss = 0.0  # Initialize validation loss\n",
    "    all_preds = []  # Store predictions\n",
    "    all_targets = []  # Store true labels\n",
    "    \n",
    "    # Evaluate on validation set without gradient computation\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move to device\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute loss\n",
    "            val_loss += loss.item() * data.size(0)  # Accumulate loss\n",
    "            _, predicted = torch.max(output, 1)  # Get predicted classes\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "            all_targets.extend(target.cpu().numpy())  # Store targets\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)  # Average loss over dataset\n",
    "    accuracy = (np.array(all_preds) == np.array(all_targets)).mean()  # Compute accuracy\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0  # Compute additional metrics\n",
    "    )\n",
    "    \n",
    "    # Return loss and metrics dictionary\n",
    "    return val_loss, {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Import Context for Flower client definition\n",
    "from flwr.common import Context\n",
    "\n",
    "# Define the client function for Flower simulation\n",
    "def client_fn(context: Context):\n",
    "    cid = int(context.node_id) % len(client_train_loaders)  # Assign client ID\n",
    "    train_loader = client_train_loaders[cid]  # Get training data for this client\n",
    "    \n",
    "    # Additional imports for client (should ideally be at the top, but placed here as per original code)\n",
    "    import torch\n",
    "    from torch import device\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    # Define the Flower client class\n",
    "    class FlowerClient:\n",
    "        def __init__(self):\n",
    "            self.model = get_model()  # Initialize model\n",
    "            self.train_loader = train_loader  # Assign training data loader\n",
    "            self.optimizer = get_optimizer(self.model)  # Initialize optimizer\n",
    "            self.criterion = get_loss_function()  # Initialize loss function\n",
    "            self.device = device  # Set device\n",
    "            # Initialize local control variate (c_k) with zeros matching model parameters\n",
    "            self.c_k = [torch.zeros_like(param).to(self.device) for param in self.model.parameters()]\n",
    "\n",
    "        def fit(self, parameters, config):\n",
    "            try:\n",
    "                # Load server parameters into the model\n",
    "                state_dict = OrderedDict({k: torch.tensor(v).to(self.device) for k, v in zip(self.model.state_dict().keys(), parameters)})\n",
    "                self.model.load_state_dict(state_dict)\n",
    "                self.model.train()  # Set model to training mode\n",
    "\n",
    "                # Extract global control variate from config (Scaffold-specific)\n",
    "                global_control_variate = config.get(\"global_control_variate\")\n",
    "                if global_control_variate is None:\n",
    "                    raise ValueError(\"Global control variate not provided in config\")\n",
    "                c = [torch.tensor(param).to(self.device) for param in global_control_variate]\n",
    "\n",
    "                # Store initial parameters for control variate update\n",
    "                initial_parameters = [param.clone().detach() for param in self.model.parameters()]\n",
    "\n",
    "                # Training loop with Scaffold's modified gradient\n",
    "                local_epochs = config.get(\"local_epochs\", 1)  # Number of local epochs\n",
    "                eta = self.optimizer.param_groups[0]['lr']  # Learning rate\n",
    "                T = local_epochs * len(self.train_loader)  # Total training steps\n",
    "\n",
    "                if T == 0:\n",
    "                    raise ValueError(\"Training loader is empty or not properly initialized\")\n",
    "\n",
    "                for epoch in range(local_epochs):\n",
    "                    for data, target in self.train_loader:\n",
    "                        data, target = data.to(self.device), target.to(self.device)  # Move to device\n",
    "                        self.optimizer.zero_grad()  # Clear gradients\n",
    "                        output = self.model(data)  # Forward pass\n",
    "                        loss = self.criterion(output, target)  # Compute loss\n",
    "                        loss.backward()  # Backpropagation\n",
    "\n",
    "                        # Modify gradients using Scaffold control variates\n",
    "                        for param, ck, gc in zip(self.model.parameters(), self.c_k, c):\n",
    "                            if param.grad.shape != ck.shape or param.grad.shape != gc.shape:\n",
    "                                raise ValueError(f\"Shape mismatch: param.grad {param.grad.shape}, ck {ck.shape}, gc {gc.shape}\")\n",
    "                            param.grad = param.grad - ck + gc  # Adjust gradient\n",
    "\n",
    "                        self.optimizer.step()  # Update model parameters\n",
    "\n",
    "                # Compute updated client control variate (c_k^+)\n",
    "                c_k_plus = [\n",
    "                    gc - (1 / (eta * T)) * (param - initial_param)\n",
    "                    for param, initial_param, gc in zip(self.model.parameters(), initial_parameters, c)\n",
    "                ]\n",
    "\n",
    "                # Update local control variate\n",
    "                self.c_k = [param.clone().detach() for param in c_k_plus]\n",
    "\n",
    "                # Return updated parameters and control variate\n",
    "                updated_params = [param.cpu().detach().numpy() for param in self.model.parameters()]\n",
    "                return updated_params, len(self.train_loader.dataset), {\"control_variate\": [param.cpu().numpy().tolist() for param in c_k_plus]}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in client fit: {str(e)}\")  # Log error\n",
    "                raise  # Re-raise for server logging\n",
    "        \n",
    "        def get_parameters(self, config):\n",
    "            return [param.cpu().detach().numpy() for param in self.model.parameters()]  # Return current parameters\n",
    "    \n",
    "    return FlowerClient().to_client()  # Convert to Flower client\n",
    "\n",
    "# Custom Scaffold strategy extending FedAvg\n",
    "class CustomScaffold(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        model = get_model()  # Initialize model to get parameter shapes\n",
    "        self.c = [np.zeros(param.shape) for param in model.parameters()]  # Global control variate\n",
    "        # Store metrics history across rounds\n",
    "        self.metrics_history = {\n",
    "            \"val_loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": []\n",
    "        }\n",
    "        self.final_parameters = None  # Store final parameters\n",
    "        self.early_stopping = EarlyStopping(patience=40, delta=0.00001, threshold=0.0001)  # Early stopping instance\n",
    "    \n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        config = super().configure_fit(server_round, parameters, client_manager)  # Default configuration\n",
    "        global_control_variate = [param.tolist() for param in self.c]  # Convert global control variate\n",
    "        for cfg in config:\n",
    "            cfg[\"config\"][\"global_control_variate\"] = global_control_variate  # Add to client config\n",
    "        return config\n",
    "    \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}  # Return if no results\n",
    "        \n",
    "        # Aggregate parameters using FedAvg\n",
    "        aggregated_parameters = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        # Aggregate client control variates\n",
    "        total_weight = sum([result.num_examples for result in results])  # Total samples\n",
    "        c_k_list = [result.metrics[\"control_variate\"] for result in results]  # Client control variates\n",
    "        c_k_arrays = [[np.array(param) for param in client_c_k] for client_c_k in c_k_list]\n",
    "        self.c = [\n",
    "            np.average([c_k[i] for c_k in c_k_arrays], weights=[result.num_examples for result in results], axis=0)\n",
    "            for i in range(len(self.c))  # Weighted average of control variates\n",
    "        ]\n",
    "        \n",
    "        return aggregated_parameters, {}  # Return aggregated parameters\n",
    "    \n",
    "    def evaluate(self, server_round, parameters):\n",
    "        result = super().evaluate(server_round, parameters)  # Evaluate global model\n",
    "        if result:\n",
    "            loss, metrics = result\n",
    "            # Store metrics\n",
    "            self.metrics_history[\"val_loss\"].append(loss)\n",
    "            for key in metrics:\n",
    "                self.metrics_history[key].append(metrics[key])\n",
    "            self.final_parameters = parameters  # Update final parameters\n",
    "            \n",
    "            # Load parameters into model for early stopping check\n",
    "            global_model = get_model()\n",
    "            final_ndarrays = parameters_to_ndarrays(parameters)\n",
    "            state_dict = {k: torch.tensor(v) for k, v in zip(global_model.state_dict().keys(), final_ndarrays)}\n",
    "            global_model.load_state_dict(state_dict)\n",
    "            \n",
    "            self.early_stopping(loss, global_model)  # Check for early stopping\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at round {server_round}.\")\n",
    "                best_state_dict = self.early_stopping.best_model_state\n",
    "                best_parameters = [best_state_dict[k].cpu().numpy() for k in global_model.state_dict().keys()]\n",
    "                self.final_parameters = best_parameters  # Update with best parameters\n",
    "                raise StopIteration(\"Early stopping triggered.\")  # Stop simulation\n",
    "        return result\n",
    "\n",
    "# Define the Scaffold strategy\n",
    "strategy = CustomScaffold(\n",
    "    fraction_fit=1.0,  # Use all clients for fitting\n",
    "    min_fit_clients=4,  # Minimum clients required for fitting\n",
    "    min_available_clients=4,  # Minimum clients available\n",
    "    evaluate_fn=evaluate_fn,  # Evaluation function\n",
    "    on_fit_config_fn=fit_config  # Fit configuration function\n",
    ")\n",
    "\n",
    "# Start the federated learning simulation\n",
    "try:\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,  # Client creation function\n",
    "        num_clients=4,  # Number of clients\n",
    "        config=fl.server.ServerConfig(num_rounds=72),  # Run for 72 rounds\n",
    "        strategy=strategy,  # Use custom Scaffold strategy\n",
    "        client_resources={\"num_cpus\": 2, \"num_gpus\": 0.5},  # Resources per client\n",
    "        ray_init_args={\n",
    "            \"num_cpus\": 16,  # Total CPUs for Ray\n",
    "            \"object_store_memory\": 40 * 1024**3  # Memory for Ray object store (40GB)\n",
    "        }\n",
    "    )\n",
    "except StopIteration as e:\n",
    "    print(e)  # Handle early stopping exception\n",
    "print(\"Federated learning simulation completed.\")\n",
    "\n",
    "# Plot metrics over rounds\n",
    "rounds = range(1, len(strategy.metrics_history['accuracy']) + 1)\n",
    "plt.figure(figsize=(12, 8))  # Set figure size\n",
    "plt.plot(rounds, strategy.metrics_history['val_loss'], label='Validation Loss')  # Plot validation loss\n",
    "plt.plot(rounds, strategy.metrics_history['accuracy'], label='Accuracy')  # Plot accuracy\n",
    "plt.plot(rounds, strategy.metrics_history['precision'], label='Precision')  # Plot precision\n",
    "plt.plot(rounds, strategy.metrics_history['recall'], label='Recall')  # Plot recall\n",
    "plt.plot(rounds, strategy.metrics_history['f1'], label='F1 Score')  # Plot F1 score\n",
    "plt.xlabel('Round')  # X-axis label\n",
    "plt.ylabel('Metric Value')  # Y-axis label\n",
    "plt.title('Federated Learning Metrics Over Rounds')  # Plot title\n",
    "plt.legend()  # Add legend\n",
    "plt.grid(True)  # Add grid\n",
    "plt.savefig('/home/ir739wb/ilyarekun/bc_project/federated-learning/fed-avg-code/scaffold-non-iid-graph.png', dpi=300)  # Save plot\n",
    "plt.close()  # Close figure\n",
    "\n",
    "# Select the best model parameters\n",
    "if strategy.early_stopping.early_stop:\n",
    "    print(\"Using the best model parameters from early stopping.\")\n",
    "    best_parameters = strategy.final_parameters  # Use early stopping parameters\n",
    "else:\n",
    "    print(\"Early stopping was not triggered. Using the final round's parameters.\")\n",
    "    best_parameters = strategy.final_parameters  # Use final round parameters\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "model = get_model()  # Create model instance\n",
    "final_ndarrays = parameters_to_ndarrays(best_parameters)  # Convert parameters\n",
    "state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), final_ndarrays)}\n",
    "model.load_state_dict(state_dict)  # Load parameters\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_preds = []  # Store predictions\n",
    "all_targets = []  # Store true labels\n",
    "with torch.no_grad():  # Disable gradients for evaluation\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move to device\n",
    "        output = model(data)  # Forward pass\n",
    "        _, predicted = torch.max(output, 1)  # Get predicted classes\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        all_targets.extend(target.cpu().numpy())  # Store targets\n",
    "\n",
    "# Compute test metrics\n",
    "accuracy = (np.array(all_preds) == np.array(all_targets)).mean()  # Accuracy\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='macro', zero_division=0)  # Additional metrics\n",
    "\n",
    "# Print test metrics\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Save metrics to a file\n",
    "metrics_file = '/home/ir739wb/ilyarekun/bc_project/federated-learning/fed-avg-code/scaffold-non-iid-metrics.txt'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    rounds = range(1, len(strategy.metrics_history['val_loss']) + 1)\n",
    "    for round_num in rounds:\n",
    "        # Write per-round validation metrics\n",
    "        f.write(f\"Round {round_num}:\\n\")\n",
    "        f.write(f\"  Validation Loss: {strategy.metrics_history['val_loss'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy: {strategy.metrics_history['accuracy'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {strategy.metrics_history['precision'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {strategy.metrics_history['recall'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  F1 Score: {strategy.metrics_history['f1'][round_num-1]:.4f}\\n\")\n",
    "    # Write test metrics\n",
    "    f.write(\"\\nTest Metrics:\\n\")\n",
    "    f.write(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"  Recall: {recall:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score: {f1:.4f}\\n\")\n",
    "print(f\"Metrics saved to '{metrics_file}'\")  # Confirm file save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
