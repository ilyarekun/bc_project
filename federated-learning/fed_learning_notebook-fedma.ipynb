{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ir739wb/ilyarekun/bc_project/federated-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for file handling, federated learning, deep learning, and data processing\n",
    "import os\n",
    "import flwr as fl  # Flower framework for federated learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import kagglehub  # Utility for downloading datasets from Kaggle\n",
    "import shutil\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "import scipy.optimize as opt  # For Hungarian algorithm in FedMA\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "# Ensure determinism in CUDA operations (may slow down training)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define device: use GPU if available, otherwise fall back to CPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the CNN model for brain tumor classification\n",
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainCNN, self).__init__()\n",
    "        # Convolutional layers: progressively extract spatial features\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),  # Input channels: 3 (RGB), output: 64\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),  # Downsample by factor of 2\n",
    "            nn.Dropout2d(0.45),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.45),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.4)\n",
    "        )\n",
    "        # Fully connected layers: map extracted features to class logits\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 1024),  # Flattened feature size: 512 channels × 3 × 3 spatial dims\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(512, 4),  # Output layer: 4 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass: apply convolutional layers, then flatten and apply FC layers\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)  # Flatten tensor to (batch_size, 512*3*3)\n",
    "        out = self.fc_layers(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Early stopping class to terminate training when validation loss plateaus or goes below a threshold\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, threshold=0.19):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of consecutive rounds without improvement before stopping.\n",
    "            delta: Minimum change in validation loss to be considered an improvement.\n",
    "            threshold: If validation loss falls below this value, stop immediately.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.threshold = threshold\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        # If validation loss is below threshold, stop immediately and save the model state\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"Val loss {val_loss:.5f} is below threshold {self.threshold}.\")\n",
    "            self.early_stop = True\n",
    "            self.best_model_state = model.state_dict()\n",
    "            return\n",
    "\n",
    "        score = -val_loss  # We maximize negative validation loss\n",
    "        if self.best_score is None:\n",
    "            # First time: initialize best score and save state\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            # No significant improvement: increase counter\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                # If patience exceeded, trigger early stopping\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Improvement found: reset counter and update best score/state\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        # Load the saved best model state into the provided model\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing function: downloads, organizes, and splits data among clients in a non-IID fashion\n",
    "def data_preprocessing_tumor_NON_IID(num_clients=4):\n",
    "    # Download the dataset from Kaggle and extract train/test into a general folder\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    train_path = os.path.join(dataset_path, \"Training\")\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)\n",
    "    \n",
    "    # Move all images from Training/Testing into the general dataset folder, preserving class subfolders\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))\n",
    "    \n",
    "    # Define transformations: center crop to 400x400, resize to 200x200, convert to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),\n",
    "        transforms.Resize((200, 200)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Load the entire dataset from the general folder\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets  # List of class labels for each image\n",
    "    classes = list(set(targets))       # Unique class labels\n",
    "    \n",
    "    # Split indices into train/validation/test sets per class\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    \n",
    "    for class_label in classes:\n",
    "        # Get all indices for this class\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        # Assign first part to train, next to val, remainder to test\n",
    "        train_indices.extend(class_indices[:train_size])\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])\n",
    "        test_indices.extend(class_indices[train_size + val_size:])\n",
    "    \n",
    "    # Create subsets for validation and test sets\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Define a fixed non-IID distribution of classes across clients\n",
    "    distribution = {\n",
    "        0: [0.70, 0.15, 0.10, 0.05],\n",
    "        1: [0.15, 0.70, 0.10, 0.05],\n",
    "        2: [0.10, 0.15, 0.70, 0.05],\n",
    "        3: [0.05, 0.10, 0.15, 0.70]\n",
    "    }\n",
    "    \n",
    "    # Initialize an empty list of indices per client\n",
    "    client_indices = {client: [] for client in range(num_clients)}\n",
    "    \n",
    "    # Distribute training indices to clients according to the distribution for each class\n",
    "    for class_label in classes:\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)\n",
    "        \n",
    "        n = len(class_train_indices)\n",
    "        allocation = []\n",
    "        # Calculate the number of samples per client for this class\n",
    "        for client in range(num_clients):\n",
    "            cnt = int(distribution[class_label][client] * n)\n",
    "            allocation.append(cnt)\n",
    "        # Ensure sum of allocated samples equals total\n",
    "        allocation[-1] = n - sum(allocation[:-1])\n",
    "        \n",
    "        start = 0\n",
    "        for client in range(num_clients):\n",
    "            cnt = allocation[client]\n",
    "            client_indices[client].extend(class_train_indices[start:start + cnt])\n",
    "            start += cnt\n",
    "    \n",
    "    # Create a DataLoader for each client using their allocated indices\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    return client_train_loaders, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Preprocess data and obtain DataLoaders for each client, validation, and test sets\n",
    "client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_NON_IID(num_clients=4)\n",
    "\n",
    "\n",
    "\n",
    "# Helper function: return a new instance of the CNN model on the appropriate device\n",
    "def get_model():\n",
    "    return BrainCNN().to(device)\n",
    "\n",
    "# Helper function: return an optimizer configured for a given model\n",
    "def get_optimizer(model):\n",
    "    return optim.SGD(model.parameters(), lr=0.001, momentum=0.7, weight_decay=0.09)\n",
    "\n",
    "# Helper function: return the loss function for classification\n",
    "def get_loss_function():\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to supply configuration to clients before fitting (e.g., number of local epochs)\n",
    "def fit_config(server_round: int):\n",
    "    return {\"local_epochs\": 5}\n",
    "\n",
    "# Evaluation function to be called by the server: computes validation loss and metrics\n",
    "def evaluate_fn(server_round, parameters, config):\n",
    "    # Instantiate and load model parameters from numpy arrays\n",
    "    model = get_model()\n",
    "    state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), parameters)}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = get_loss_function()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Compute loss and predictions on the validation set\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)  # Average validation loss\n",
    "    accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Return validation loss and a dictionary of metrics\n",
    "    return val_loss, {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Client function: defines the behavior of each client in the federated simulation\n",
    "from flwr.common import Context\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    # Determine client ID and assign the appropriate DataLoader\n",
    "    cid = int(context.node_id) % len(client_train_loaders)\n",
    "    train_loader = client_train_loaders[cid]\n",
    "    \n",
    "    import torch\n",
    "    from torch import device\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    class FlowerClient:\n",
    "        def __init__(self):\n",
    "            self.model = get_model()\n",
    "            self.train_loader = train_loader\n",
    "            self.optimizer = get_optimizer(self.model)\n",
    "            self.criterion = get_loss_function()\n",
    "            self.device = device\n",
    "\n",
    "        def fit(self, parameters, config):\n",
    "            try:\n",
    "                # Load incoming global model parameters\n",
    "                state_dict = OrderedDict({k: torch.tensor(v).to(self.device) for k, v in zip(self.model.state_dict().keys(), parameters)})\n",
    "                self.model.load_state_dict(state_dict)\n",
    "                self.model.train()\n",
    "\n",
    "                local_epochs = config.get(\"local_epochs\", 1)\n",
    "                # Perform local training for the specified number of epochs\n",
    "                for epoch in range(local_epochs):\n",
    "                    for data, target in self.train_loader:\n",
    "                        data, target = data.to(self.device), target.to(self.device)\n",
    "                        self.optimizer.zero_grad()\n",
    "                        output = self.model(data)\n",
    "                        loss = self.criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                # Return updated parameters and number of training examples\n",
    "                updated_params = [param.cpu().detach().numpy() for param in self.model.parameters()]\n",
    "                return updated_params, len(self.train_loader.dataset), {}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in client fit: {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "        def get_parameters(self, config):\n",
    "            # Return the current model parameters without training\n",
    "            return [param.cpu().detach().numpy() for param in self.model.parameters()]\n",
    "    \n",
    "    # Return the client wrapped as a FlowerClient\n",
    "    return FlowerClient().to_client()\n",
    "\n",
    "\n",
    "\n",
    "# Custom FedMA strategy: implements federated matching and averaging to align neurons/filters\n",
    "class CustomFedMA(fl.server.strategy.Strategy):\n",
    "    def __init__(self, fraction_fit=1.0, min_fit_clients=4, min_available_clients=4, evaluate_fn=None, on_fit_config_fn=None):\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "        self.metrics_history = {\n",
    "            \"val_loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": []\n",
    "        }\n",
    "        self.final_parameters = None\n",
    "        # Use early stopping with high patience for federated rounds\n",
    "        self.early_stopping = EarlyStopping(patience=40, delta=0.00001, threshold=0.0001)\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        \"\"\"\n",
    "        Select which clients to run the next fit round on and package the FitIns messages.\n",
    "        \"\"\"\n",
    "        config = {}\n",
    "        if self.on_fit_config_fn is not None:\n",
    "            config = self.on_fit_config_fn(server_round)\n",
    "        sample_size, min_num_clients = self.num_fit_clients(client_manager.num_available())\n",
    "        clients = client_manager.sample(num_clients=sample_size, min_num_clients=min_num_clients)\n",
    "        return [(client, fl.common.FitIns(parameters, config)) for client in clients]\n",
    "\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        \"\"\"\n",
    "        Aggregate model updates from clients using neuron/filter matching (Hungarian algorithm).\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        # Convert Flower parameters back to numpy arrays for each client\n",
    "        client_params = [parameters_to_ndarrays(result.parameters) for result in results]\n",
    "        num_examples = [result.num_examples for result in results]\n",
    "        total_examples = sum(num_examples)\n",
    "\n",
    "        # Instantiate a fresh model to inspect layer shapes\n",
    "        model = get_model()\n",
    "        param_shapes = [param.shape for param in model.parameters()]\n",
    "        aggregated_params = []\n",
    "\n",
    "        # Iterate over each layer in the model\n",
    "        for layer_idx, shape in enumerate(param_shapes):\n",
    "            # Collect this layer's parameters from all clients\n",
    "            layer_params = [client[layer_idx] for client in client_params]\n",
    "\n",
    "            if len(shape) == 1:\n",
    "                # Bias vector: simple weighted average\n",
    "                aggregated_layer = np.average(layer_params, weights=num_examples, axis=0)\n",
    "            else:\n",
    "                if len(shape) == 2:\n",
    "                    # Fully connected layer: match neurons across clients\n",
    "                    num_neurons = shape[0]\n",
    "                    cost_matrix = np.zeros((num_neurons, num_neurons))\n",
    "                    # Compute cost between every pair of neurons by average L2 distance\n",
    "                    for i in range(num_neurons):\n",
    "                        for j in range(num_neurons):\n",
    "                            cost_matrix[i, j] = np.mean([\n",
    "                                np.linalg.norm(p1[i] - p2[j]) \n",
    "                                for p1 in layer_params \n",
    "                                for p2 in layer_params if p1 is not p2\n",
    "                            ])\n",
    "                    # Solve assignment problem to align neurons\n",
    "                    row_ind, col_ind = opt.linear_sum_assignment(cost_matrix)\n",
    "                    matched_params = np.mean([p[row_ind] for p in layer_params], axis=0)\n",
    "                    aggregated_layer = matched_params\n",
    "\n",
    "                elif len(shape) == 4:\n",
    "                    # Convolutional layer: match filters across clients\n",
    "                    num_filters = shape[0]\n",
    "                    cost_matrix = np.zeros((num_filters, num_filters))\n",
    "                    # Compute cost between every pair of filters by average L2 distance\n",
    "                    for i in range(num_filters):\n",
    "                        for j in range(num_filters):\n",
    "                            cost_matrix[i, j] = np.mean([\n",
    "                                np.linalg.norm(p1[i].flatten() - p2[j].flatten())\n",
    "                                for p1 in layer_params\n",
    "                                for p2 in layer_params if p1 is not p2\n",
    "                            ])\n",
    "                    # Solve assignment problem to align filters\n",
    "                    row_ind, col_ind = opt.linear_sum_assignment(cost_matrix)\n",
    "                    matched_params = np.mean([p[row_ind] for p in layer_params], axis=0)\n",
    "                    aggregated_layer = matched_params\n",
    "\n",
    "                else:\n",
    "                    # Default to weighted average for other parameter shapes\n",
    "                    aggregated_layer = np.average(layer_params, weights=num_examples, axis=0)\n",
    "\n",
    "            aggregated_params.append(aggregated_layer)\n",
    "\n",
    "        return aggregated_params, {}\n",
    "\n",
    "    def evaluate(self, server_round, parameters):\n",
    "        \"\"\"\n",
    "        Evaluate global model on validation data and optionally trigger early stopping.\n",
    "        \"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            return None\n",
    "\n",
    "        result = self.evaluate_fn(server_round, parameters, {})\n",
    "        if result:\n",
    "            loss, metrics = result\n",
    "            # Record metrics history\n",
    "            self.metrics_history[\"val_loss\"].append(loss)\n",
    "            for key in metrics:\n",
    "                self.metrics_history[key].append(metrics[key])\n",
    "            self.final_parameters = parameters\n",
    "            \n",
    "            # Load global model to check for early stopping\n",
    "            global_model = get_model()\n",
    "            final_ndarrays = parameters_to_ndarrays(parameters)\n",
    "            state_dict = {k: torch.tensor(v) for k, v in zip(global_model.state_dict().keys(), final_ndarrays)}\n",
    "            global_model.load_state_dict(state_dict)\n",
    "            \n",
    "            # Perform early stopping check\n",
    "            self.early_stopping(loss, global_model)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at round {server_round}.\")\n",
    "                best_state_dict = self.early_stopping.best_model_state\n",
    "                # Convert best state dict back to numpy parameters\n",
    "                best_parameters = [best_state_dict[k].cpu().numpy() for k in global_model.state_dict().keys()]\n",
    "                self.final_parameters = best_parameters\n",
    "                # Raise StopIteration to halt simulation\n",
    "                raise StopIteration(\"Early stopping triggered.\")\n",
    "        return result\n",
    "\n",
    "    def num_fit_clients(self, num_available):\n",
    "        \"\"\"\n",
    "        Determine how many clients to sample for fitting based on fraction_fit and minimums.\n",
    "        \"\"\"\n",
    "        num_clients = int(num_available * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "\n",
    "\n",
    "# Define the FedMA strategy with specified parameters\n",
    "strategy = CustomFedMA(\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=4,\n",
    "    min_available_clients=4,\n",
    "    evaluate_fn=evaluate_fn,\n",
    "    on_fit_config_fn=fit_config\n",
    ")\n",
    "\n",
    "# Start the federated learning simulation\n",
    "try:\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=4,\n",
    "        config=fl.server.ServerConfig(num_rounds=72),  # Total federated rounds\n",
    "        strategy=strategy,\n",
    "        client_resources={\"num_cpus\": 2, \"num_gpus\": 0.5},\n",
    "        ray_init_args={\n",
    "            \"num_cpus\": 16,\n",
    "            \"object_store_memory\": 40 * 1024**3\n",
    "        }\n",
    "    )\n",
    "except StopIteration as e:\n",
    "    # Catch early stopping signal to exit simulation gracefully\n",
    "    print(e)\n",
    "\n",
    "print(\"Federated learning simulation completed.\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot metrics recorded over federated rounds\n",
    "rounds = range(1, len(strategy.metrics_history['accuracy']) + 1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(rounds, strategy.metrics_history['val_loss'], label='Validation Loss')\n",
    "plt.plot(rounds, strategy.metrics_history['accuracy'], label='Accuracy')\n",
    "plt.plot(rounds, strategy.metrics_history['precision'], label='Precision')\n",
    "plt.plot(rounds, strategy.metrics_history['recall'], label='Recall')\n",
    "plt.plot(rounds, strategy.metrics_history['f1'], label='F1 Score')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Federated Learning Metrics Over Rounds (FedMA)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Save the plot to a file\n",
    "plt.savefig('/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fedma-non-iid-graph.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# After simulation, select best parameters based on early stopping\n",
    "if strategy.early_stopping.early_stop:\n",
    "    print(\"Using the best model parameters from early stopping.\")\n",
    "    best_parameters = strategy.final_parameters\n",
    "else:\n",
    "    print(\"Early stopping was not triggered. Using the final round's parameters.\")\n",
    "    best_parameters = strategy.final_parameters\n",
    "\n",
    "# Load the best global model and evaluate on the test set\n",
    "model = get_model()\n",
    "final_ndarrays = parameters_to_ndarrays(best_parameters)\n",
    "state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), final_ndarrays)}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "# Compute and print final test metrics\n",
    "accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Save all recorded metrics (per round and final test) to a text file\n",
    "metrics_file = '/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fedma-non-iid-metrics.txt'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    rounds = range(1, len(strategy.metrics_history['val_loss']) + 1)\n",
    "    for round_num in rounds:\n",
    "        f.write(f\"Round {round_num}:\\n\")\n",
    "        f.write(f\"  Validation Loss: {strategy.metrics_history['val_loss'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy: {strategy.metrics_history['accuracy'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {strategy.metrics_history['precision'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {strategy.metrics_history['recall'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  F1 Score: {strategy.metrics_history['f1'][round_num-1]:.4f}\\n\")\n",
    "    f.write(\"\\nTest Metrics:\\n\")\n",
    "    f.write(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"  Recall: {recall:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to '{metrics_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
