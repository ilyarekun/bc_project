{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ir739wb/ilyarekun/bc_project/federated-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import shutil\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.45),\n",
    "            nn.Conv2d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.45),\n",
    "            nn.Conv2d(256, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "            nn.Conv2d(256, 512, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(512, 4),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)  # Flatten\n",
    "        out = self.fc_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping utility to stop training when validation loss stops improving\n",
    "    or reaches a specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=0, threshold=0.19):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last improvement before stopping.\n",
    "            delta (float): Minimum change in validation loss to consider as an improvement.\n",
    "            threshold (float): Absolute validation loss threshold; stop immediately if reached or below.\n",
    "        \"\"\"\n",
    "        self.patience = patience              # How many epochs to wait for improvement\n",
    "        self.delta = delta                    # Minimum improvement in validation loss\n",
    "        self.threshold = threshold            # Immediate stop if val_loss <= threshold\n",
    "        self.best_score = None                # Best score seen so far (negative val_loss)\n",
    "        self.early_stop = False               # Flag to indicate stopping\n",
    "        self.counter = 0                      # Counter for epochs without improvement\n",
    "        self.best_model_state = None          # State dict of the best model\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Call method to check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Current validation loss.\n",
    "            model (nn.Module): Model being trained; used to save best state.\n",
    "        \"\"\"\n",
    "        # If validation loss is below or equal to the threshold, stop immediately\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"Val loss {val_loss:.5f} is below threshold {self.threshold}. Stopping training.\")\n",
    "            self.early_stop = True\n",
    "            # Save the current model state as the best\n",
    "            self.best_model_state = model.state_dict()\n",
    "            return\n",
    "\n",
    "        # Convert validation loss to a score (we want to maximize -val_loss)\n",
    "        score = -val_loss\n",
    "\n",
    "        # If this is the first call, initialize best_score and best_model_state\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        # If no sufficient improvement, increment counter\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            # If patience exceeded, set early_stop flag\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Improvement found: update best_score and reset counter\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Load the best saved model state into the provided model.\n",
    "        \n",
    "        Args:\n",
    "            model (nn.Module): Model into which to load the best state.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "\n",
    "def data_preprocessing_tumor_IID(num_clients=4):\n",
    "    \"\"\"\n",
    "    Download the brain tumor MRI dataset from Kaggle, merge training and testing folders\n",
    "    into a single directory, perform a stratified train/val/test split, and then\n",
    "    partition the training data evenly (IID) among a specified number of clients.\n",
    "    \n",
    "    Args:\n",
    "        num_clients (int): Number of clients to split the training set into. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "        client_train_loaders (list of DataLoader): List of DataLoader objects, one per client.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "    \"\"\"\n",
    "    # Download the dataset from Kaggle and get the local path\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    \n",
    "    # Define paths to the original Training and Testing folders\n",
    "    train_path = os.path.join(dataset_path, \"Training\")\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")\n",
    "    \n",
    "    # Create a new directory to hold all images together (General_Dataset)\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)\n",
    "    \n",
    "    # Move images from Training and Testing into General_Dataset, preserving class subfolders\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))\n",
    "    \n",
    "    # Define image transformations: center crop, resize, then convert to tensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),  # Crop the image to 400x400 pixels from the center\n",
    "        transforms.Resize((200, 200)),      # Resize cropped image to 200x200 pixels\n",
    "        transforms.ToTensor(),              # Convert PIL Image to PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Load all images from General_Dataset via ImageFolder (expects subfolders = class names)\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets             # List of class labels for each image\n",
    "    classes = list(set(targets))                  # Unique class labels present in the dataset\n",
    "    \n",
    "    # Initialize lists to hold indices for train/val/test splits\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Ratios for splitting each class: 70% train, 20% validation, 10% test\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "    \n",
    "    # Perform a stratified split: for each class, allocate indices proportionally\n",
    "    for class_label in classes:\n",
    "        # Collect indices of all samples belonging to the current class\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        \n",
    "        # Determine how many samples go to train, val, and test\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        test_size = class_size - train_size - val_size  # Remainder goes to test\n",
    "        \n",
    "        # Assign the first train_size indices to the training split\n",
    "        train_indices.extend(class_indices[:train_size])\n",
    "        # Assign the next val_size indices to the validation split\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])\n",
    "        # Assign the remaining indices to the test split\n",
    "        test_indices.extend(class_indices[train_size + val_size:])\n",
    "    \n",
    "    # Create Subset objects for each split using the computed indices\n",
    "    train_set = Subset(general_dataset, train_indices)\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    \n",
    "    # DataLoader for validation and test sets (no shuffling needed)\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Prepare to split the training set into num_clients disjoint subsets (IID)\n",
    "    # Initialize a dictionary mapping client_id -> list of sample indices\n",
    "    client_indices = {client: [] for client in range(num_clients)}\n",
    "    \n",
    "    # For each class, shuffle its training indices and split them equally among clients\n",
    "    for class_label in classes:\n",
    "        # Get all training indices that belong to the current class\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)  # Shuffle to randomize assignment\n",
    "        \n",
    "        # Split this class's indices into num_clients roughly equal parts\n",
    "        splits = np.array_split(class_train_indices, num_clients)\n",
    "        \n",
    "        # Assign each split to the corresponding client's index list\n",
    "        for client in range(num_clients):\n",
    "            client_indices[client].extend(splits[client].tolist())\n",
    "    \n",
    "    # Create a DataLoader for each client's subset of the training data\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    # Return the list of client-specific train loaders, and shared val/test loaders\n",
    "    return client_train_loaders, val_loader, test_loader\n",
    "\n",
    "# Example usage (commented out on purpose):\n",
    "# client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_IID(num_clients=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing_tumor_NON_IID(num_clients=4):\n",
    "    # Download and prepare the dataset\n",
    "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
    "    train_path = os.path.join(dataset_path, \"Training\")\n",
    "    test_path = os.path.join(dataset_path, \"Testing\")\n",
    "    general_dataset_path = os.path.join(dataset_path, \"General_Dataset\")\n",
    "    os.makedirs(general_dataset_path, exist_ok=True)\n",
    "    \n",
    "    # Merge Training and Testing folders into a single General_Dataset folder by class\n",
    "    for source_path in [train_path, test_path]:\n",
    "        for class_name in os.listdir(source_path):\n",
    "            class_path = os.path.join(source_path, class_name)\n",
    "            general_class_path = os.path.join(general_dataset_path, class_name)\n",
    "            os.makedirs(general_class_path, exist_ok=True)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                shutil.move(img_path, os.path.join(general_class_path, img_name))\n",
    "    \n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((400, 400)),\n",
    "        transforms.Resize((200, 200)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create the PyTorch dataset from the general dataset folder\n",
    "    general_dataset = ImageFolder(root=general_dataset_path, transform=transform)\n",
    "    targets = general_dataset.targets\n",
    "    classes = list(set(targets))\n",
    "    \n",
    "    # Split indices into train/validation/test sets (stratified by class)\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    # test_ratio will be the remainder (0.1)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        class_indices = [i for i, target in enumerate(targets) if target == class_label]\n",
    "        class_size = len(class_indices)\n",
    "        train_size = int(train_ratio * class_size)\n",
    "        val_size = int(val_ratio * class_size)\n",
    "        # Assign the first portion for training, next for validation, rest for test\n",
    "        train_indices.extend(class_indices[:train_size])\n",
    "        val_indices.extend(class_indices[train_size:train_size + val_size])\n",
    "        test_indices.extend(class_indices[train_size + val_size:])\n",
    "    \n",
    "    # Create subsets and dataloaders for validation and test sets\n",
    "    train_set = Subset(general_dataset, train_indices)\n",
    "    val_set = Subset(general_dataset, val_indices)\n",
    "    test_set = Subset(general_dataset, test_indices)\n",
    "    \n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # ===== Non-IID Train Data Partitioning =====\n",
    "    # Distribution table (for 4 classes and 4 clients):\n",
    "    # For class 0: client0:70%, client1:15%, client2:10%, client3:5%\n",
    "    # For class 1: client0:15%, client1:70%, client2:10%, client3:5%\n",
    "    # For class 2: client0:10%, client1:15%, client2:70%, client3:5%\n",
    "    # For class 3: client0:5%,  client1:10%, client2:15%, client3:70%\n",
    "    # The keys are class labels and each value is a list of percentages for each client.\n",
    "    distribution = {\n",
    "        0: [0.70, 0.15, 0.10, 0.05],\n",
    "        1: [0.15, 0.70, 0.10, 0.05],\n",
    "        2: [0.10, 0.15, 0.70, 0.05],\n",
    "        3: [0.05, 0.10, 0.15, 0.70]\n",
    "    }\n",
    "    \n",
    "    # Initialize a dictionary to hold the train indices for each client\n",
    "    client_indices = {client: [] for client in range(num_clients)}\n",
    "    \n",
    "    # For each class, distribute the training samples among clients according to the distribution\n",
    "    for class_label in classes:\n",
    "        # Get all training indices for the given class\n",
    "        class_train_indices = [idx for idx in train_indices if general_dataset.targets[idx] == class_label]\n",
    "        np.random.shuffle(class_train_indices)\n",
    "        \n",
    "        n = len(class_train_indices)\n",
    "        # Compute allocation counts for each client for this class\n",
    "        allocation = []\n",
    "        for client in range(num_clients):\n",
    "            cnt = int(distribution[class_label][client] * n)\n",
    "            allocation.append(cnt)\n",
    "        # Adjust the last client allocation to account for rounding errors\n",
    "        allocation[-1] = n - sum(allocation[:-1])\n",
    "        \n",
    "        start = 0\n",
    "        for client in range(num_clients):\n",
    "            cnt = allocation[client]\n",
    "            client_indices[client].extend(class_train_indices[start:start + cnt])\n",
    "            start += cnt\n",
    "    \n",
    "    # Create DataLoaders for each client's training subset\n",
    "    client_train_loaders = []\n",
    "    for client in range(num_clients):\n",
    "        subset = Subset(general_dataset, client_indices[client])\n",
    "        loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "        client_train_loaders.append(loader)\n",
    "    \n",
    "    return client_train_loaders, val_loader, test_loader\n",
    "\n",
    "# Compute data loaders for 4 clients\n",
    "client_train_loaders, val_loader, test_loader = data_preprocessing_tumor_NON_IID(num_clients=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Instantiate a new BrainCNN model and move it to the configured device (CPU or GPU).\n",
    "    \"\"\"\n",
    "    return BrainCNN().to(device)\n",
    "\n",
    "\n",
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Create and return an optimizer for training the provided model.\n",
    "    \n",
    "    Notes on experiment history (do not modify these commented lines):\n",
    "        _: lr = 0.001, weight_decay=0.001\n",
    "        #return optim.SGD(model.parameters(), lr=0.0008, momentum=0.9, weight_decay=0.09)\n",
    "        #return optim.SGD(model.parameters(), lr=0.0007, momentum=0.9, weight_decay=0.09)\n",
    "        #return optim.SGD(model.parameters(), lr=0.0009, momentum=0.9, weight_decay=0.05)\n",
    "        #return optim.SGD(model.parameters(), lr=0.0009, momentum=0.8, weight_decay=0.07) -- iid final\n",
    "        #return optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.09) - 1\n",
    "    The active configuration below was chosen for non-IID experiments:\n",
    "    \"\"\"\n",
    "    return optim.SGD(model.parameters(), lr=0.0008, momentum=0.7, weight_decay=0.09)  # -- non iid final\n",
    "\n",
    "\n",
    "def get_loss_function():\n",
    "    \"\"\"\n",
    "    Return the loss function to be used during training and evaluation.\n",
    "    Using CrossEntropyLoss for multi-class classification.\n",
    "    \"\"\"\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def fit_config(server_round: int):\n",
    "    \"\"\"\n",
    "    Return training configuration for federated learning.\n",
    "    In this case, each client will perform 5 local epochs per round.\n",
    "    \n",
    "    Args:\n",
    "        server_round (int): Current round index on the server (unused here).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains \"local_epochs\" to be used by Flower.\n",
    "    \"\"\"\n",
    "    return {\"local_epochs\": 5}  # 5 local epochs per client per round\n",
    "\n",
    "\n",
    "def evaluate_fn(server_round, parameters, config):\n",
    "    \"\"\"\n",
    "    Evaluate the global model on the validation set after receiving updated parameters.\n",
    "    \n",
    "    Args:\n",
    "        server_round (int): Current federated learning round (unused here but included by Flower API).\n",
    "        parameters (list of np.ndarray): Model weights from the server to load into a fresh model.\n",
    "        config (dict): Additional configuration (unused here).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - val_loss (float): The average loss on the validation dataset.\n",
    "            - metrics (dict): Dictionary with keys \"accuracy\", \"precision\", \"recall\", \"f1\".\n",
    "    \n",
    "    Procedure:\n",
    "        1. Recreate a new model and load the parameters into it.\n",
    "        2. Set the model to evaluation mode.\n",
    "        3. Iterate through the global val_loader (assumed defined globally), accumulate loss and predictions.\n",
    "        4. Compute average loss and classification metrics.\n",
    "    \"\"\"\n",
    "    # 1. Instantiate a fresh model and load received parameters\n",
    "    model = get_model()\n",
    "    # Map the list of ndarrays back to the model's state_dict\n",
    "    state_dict = {k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), parameters)}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # Switch to evaluation mode (disable dropout, batchnorm updates, etc.)\n",
    "\n",
    "    # 2. Set up loss function\n",
    "    criterion = get_loss_function()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # 3. Iterate over validation batches (assumes val_loader is defined globally)\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)  # Accumulate weighted by batch size\n",
    "\n",
    "            # Get predicted class indices\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # 4. Compute average validation loss across all samples\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    # 5. Compute accuracy, precision, recall, and F1 (macro-averaged)\n",
    "    accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Return loss and a dictionary of metrics\n",
    "    return val_loss, {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import Context\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\"\n",
    "    Factory function to create a Flower client instance based on the node context.\n",
    "    Args:\n",
    "        context (Context): Flower-provided context containing node ID and configuration.\n",
    "    Returns:\n",
    "        fl.client.NumPyClient: A Flower client configured for the appropriate local data.\n",
    "    \"\"\"\n",
    "    # Determine which client index to use based on the context's node_id.\n",
    "    # We mod by the number of available client_train_loaders to cycle through them.\n",
    "    # Uncomment alternatives if you want to extract client ID differently:\n",
    "    # cid = context.node_config[\"cid\"]\n",
    "    # cid = context.node_id\n",
    "    cid = int(context.node_id) % len(client_train_loaders)\n",
    "\n",
    "    # Initialize a fresh BrainCNN model and move it to the configured device (CPU/GPU).\n",
    "    model = BrainCNN().to(device)\n",
    "    # Create an optimizer for this model using the chosen hyperparameters.\n",
    "    optimizer = get_optimizer(model)\n",
    "    # Define the loss function (CrossEntropyLoss in this case).\n",
    "    criterion = get_loss_function()\n",
    "    # Select the local training DataLoader corresponding to this client ID.\n",
    "    train_loader = client_train_loaders[int(cid)]\n",
    "    \n",
    "    class FlowerClient(fl.client.NumPyClient):\n",
    "        \"\"\"\n",
    "        Implementation of the Flower NumPyClient interface for federated learning.\n",
    "        Each method below defines how the client handles parameter exchange,\n",
    "        local training (fit), and local evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_parameters(self, config):\n",
    "            \"\"\"\n",
    "            Return the current local model parameters as a list of NumPy arrays.\n",
    "            Flower will send these arrays to the server.\n",
    "            Args:\n",
    "                config (dict): Configuration dictionary provided by the server (unused here).\n",
    "            Returns:\n",
    "                List[np.ndarray]: Model weights converted to NumPy arrays.\n",
    "            \"\"\"\n",
    "            return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "        def fit(self, parameters, config):\n",
    "            \"\"\"\n",
    "            Perform local training on this client's data.\n",
    "            Args:\n",
    "                parameters (List[np.ndarray]): Global model weights from the server.\n",
    "                config (dict): Configuration dictionary (includes \"local_epochs\").\n",
    "            Returns:\n",
    "                Tuple[List[np.ndarray], int, dict]: \n",
    "                    - Updated model parameters,\n",
    "                    - Number of training examples used,\n",
    "                    - Optional metrics dictionary (empty here).\n",
    "            \"\"\"\n",
    "            # Load the global parameters into the local model\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v) \n",
    "                for k, v in zip(model.state_dict().keys(), parameters)\n",
    "            }\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.train()  # Switch to training mode\n",
    "\n",
    "            # Run the specified number of local epochs\n",
    "            for _ in range(config[\"local_epochs\"]):\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # After training, return updated weights and dataset size\n",
    "            return self.get_parameters(config), len(train_loader.dataset), {}\n",
    "\n",
    "        def evaluate(self, parameters, config):\n",
    "            \"\"\"\n",
    "            Perform local evaluation on this client's training set (used as a proxy here).\n",
    "            Args:\n",
    "                parameters (List[np.ndarray]): Global model weights from the server.\n",
    "                config (dict): Configuration dictionary (unused for evaluation).\n",
    "            Returns:\n",
    "                Tuple[float, int, dict]:\n",
    "                    - Average loss over the local dataset,\n",
    "                    - Number of examples used for evaluation,\n",
    "                    - Metrics dictionary (accuracy placeholder here).\n",
    "            \"\"\"\n",
    "            # Load the server-provided parameters into the local model\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v)\n",
    "                for k, v in zip(model.state_dict().keys(), parameters)\n",
    "            }\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "\n",
    "            loss = 0.0\n",
    "            num_examples = 0\n",
    "\n",
    "            # Accumulate loss over all local data\n",
    "            with torch.no_grad():\n",
    "                for data, target in train_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss += criterion(output, target).item()\n",
    "                    num_examples += data.size(0)  # Track total examples count\n",
    "\n",
    "            # Compute average loss; avoid division by zero if loader is empty\n",
    "            avg_loss = loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "            metrics = {\"accuracy\": 0.0}  # Placeholder; replace with actual accuracy if desired\n",
    "\n",
    "            return avg_loss, num_examples, metrics\n",
    "\n",
    "    # Instantiate and return the Flower client object for this node\n",
    "    return FlowerClient().to_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFedAvg(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Custom federated averaging strategy that extends Flower's FedAvg and adds:\n",
    "    - Tracking of validation metrics history\n",
    "    - Early stopping based on validation loss threshold\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Initialize a history dictionary to store validation loss and other metrics over rounds\n",
    "        self.metrics_history = {\n",
    "            \"val_loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": []\n",
    "        }\n",
    "        # Will hold the parameters of the model at the last evaluated round (or best if early stopped)\n",
    "        self.final_parameters = None\n",
    "        # Set up an EarlyStopping instance with high patience and a small delta for fine-grained stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=40,        # Number of rounds to wait without improvement before stopping\n",
    "            delta=0.00001,      # Minimum change in validation loss to count as improvement\n",
    "            threshold=0.0001    # Absolute threshold for validation loss to stop immediately\n",
    "        )\n",
    "\n",
    "    def evaluate(self, server_round, parameters):\n",
    "        \"\"\"\n",
    "        Override the FedAvg evaluate method to:\n",
    "        1. Call the parent evaluate to get (val_loss, metrics) for the global model\n",
    "        2. Append those to metrics_history\n",
    "        3. Load the global model with current parameters and check early stopping\n",
    "        4. If early stopping is triggered, save best parameters and raise StopIteration\n",
    "        \"\"\"\n",
    "        # Call the base class evaluate, which will use the provided evaluate_fn to compute loss & metrics\n",
    "        result = super().evaluate(server_round, parameters)\n",
    "        \n",
    "        # If evaluation returned something (i.e., there was a validation set and clients returned metrics)\n",
    "        if result:\n",
    "            loss, metrics = result\n",
    "            \n",
    "            # Append the new metrics to the history lists\n",
    "            self.metrics_history[\"val_loss\"].append(loss)\n",
    "            for key in metrics:\n",
    "                self.metrics_history[key].append(metrics[key])\n",
    "            \n",
    "            # Save the current parameters as the most recent parameters\n",
    "            self.final_parameters = parameters\n",
    "            \n",
    "            # Recreate a fresh global model and load it with the current parameters to check early stopping\n",
    "            global_model = get_model()  # get_model() returns a new BrainCNN instance on the correct device\n",
    "            # Flower passes parameters as a list of numpy arrays; convert them to torch Tensors and build state_dict\n",
    "            final_ndarrays = parameters_to_ndarrays(parameters)\n",
    "            state_dict = {\n",
    "                k: torch.tensor(v) \n",
    "                for k, v in zip(global_model.state_dict().keys(), final_ndarrays)\n",
    "            }\n",
    "            global_model.load_state_dict(state_dict)\n",
    "            \n",
    "            # Call early stopping with the current validation loss and the freshly loaded model\n",
    "            self.early_stopping(loss, global_model)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at server round {server_round}.\")\n",
    "                # Retrieve the best model state from early_stopping\n",
    "                best_state_dict = self.early_stopping.best_model_state\n",
    "                # Convert that state_dict into a list of NumPy arrays in the same key order as the model\n",
    "                best_parameters = [\n",
    "                    best_state_dict[k].cpu().numpy() \n",
    "                    for k in global_model.state_dict().keys()\n",
    "                ]\n",
    "                # Update final_parameters to point to the best weights found so far\n",
    "                self.final_parameters = best_parameters\n",
    "                # Raise StopIteration to instruct Flower to halt training early\n",
    "                raise StopIteration(\"Early stopping triggered.\")\n",
    "        \n",
    "        # Return whatever the base class returned (loss and metrics) if no early stop\n",
    "        return result\n",
    "\n",
    "\n",
    "# Instantiate the custom strategy with required arguments for federated training:\n",
    "strategy = CustomFedAvg(\n",
    "    fraction_fit=1.0,       # Use 100% of available clients for training each round\n",
    "    min_fit_clients=4,      # Minimum number of clients to be sampled for training\n",
    "    min_available_clients=4,# Minimum number of clients that must be connected to start training\n",
    "    evaluate_fn=evaluate_fn,  # Function to evaluate global model on validation data\n",
    "    on_fit_config_fn=fit_config # Function to pass configuration (e.g., local epochs) to clients\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start a federated learning simulation using Flower's simulation API\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,  # Function to create new client instances\n",
    "        num_clients=4,  # Total number of federated clients to simulate\n",
    "        config=fl.server.ServerConfig(num_rounds=72),  # Server configuration: run for 72 rounds\n",
    "        strategy=strategy,  # Custom FedAvg strategy instance (handles aggregation, evaluation, early stopping)\n",
    "        client_resources={\"num_cpus\": 2, \"num_gpus\": 0.5},  \n",
    "            # Resources to allocate per client: 2 CPU cores and 0.5 GPU\n",
    "        ray_init_args={\n",
    "            \"num_cpus\": 16,  \n",
    "                # Total number of CPU cores available to Ray for parallel client simulation\n",
    "            \"object_store_memory\": 40 * 1024**3  \n",
    "                # Amount of memory (in bytes) for Ray's object store (40 GiB)\n",
    "        }\n",
    "    )\n",
    "except StopIteration as e:\n",
    "    # Catch StopIteration raised by early stopping in the custom strategy\n",
    "    print(e)  # Print reason for early stopping\n",
    "# This line will execute after the simulation finishes (or is stopped early)\n",
    "print(\"Federated learning simulation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x-axis values as rounds, starting from 1 up to the number of recorded accuracy values\n",
    "rounds = range(1, len(strategy.metrics_history['accuracy']) + 1)\n",
    "\n",
    "# Create a new figure with a specified size (width=12 inches, height=8 inches)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot validation loss over rounds\n",
    "plt.plot(rounds, strategy.metrics_history['val_loss'], label='Validation Loss')\n",
    "\n",
    "# Plot accuracy over rounds\n",
    "plt.plot(rounds, strategy.metrics_history['accuracy'], label='Accuracy')\n",
    "\n",
    "# Plot precision over rounds\n",
    "plt.plot(rounds, strategy.metrics_history['precision'], label='Precision')\n",
    "\n",
    "# Plot recall over rounds\n",
    "plt.plot(rounds, strategy.metrics_history['recall'], label='Recall')\n",
    "\n",
    "# Plot F1 score over rounds\n",
    "plt.plot(rounds, strategy.metrics_history['f1'], label='F1 Score')\n",
    "\n",
    "# Label the x-axis as 'Round'\n",
    "plt.xlabel('Round')\n",
    "\n",
    "# Label the y-axis as 'Metric Value'\n",
    "plt.ylabel('Metric Value')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Federated Learning Metrics Over Rounds')\n",
    "\n",
    "# Display a legend to identify each plotted metric\n",
    "plt.legend()\n",
    "\n",
    "# Show a grid to improve readability of the plot\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure to the specified file path at 300 DPI resolution\n",
    "plt.savefig(\n",
    "    '/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fed-avg-non-iid-graph2.png',\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Display the plot window (if running in an interactive environment)\n",
    "plt.show()\n",
    "\n",
    "# Close the figure to release memory\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import parameters_to_ndarrays\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Determine which set of model parameters to use:\n",
    "# If early stopping was triggered, use the best parameters saved by early stopping.\n",
    "# Otherwise, use the final parameters from the last federated round.\n",
    "if strategy.early_stopping.early_stop:\n",
    "    print(\"Using the best model parameters from early stopping.\")\n",
    "    best_parameters = strategy.final_parameters\n",
    "else:\n",
    "    print(\"Early stopping was not triggered. Using the final round's parameters.\")\n",
    "    best_parameters = strategy.final_parameters\n",
    "\n",
    "# Create a fresh model instance (BrainCNN on the chosen device)\n",
    "model = get_model()  # Assumes get_model() returns BrainCNN().to(device)\n",
    "\n",
    "# Convert Flower parameters (which may be wrapped) into a list of NumPy arrays\n",
    "final_ndarrays = parameters_to_ndarrays(best_parameters)\n",
    "\n",
    "# Build a state_dict mapping each key in the model to its corresponding torch.Tensor\n",
    "state_dict = {\n",
    "    k: torch.tensor(v)\n",
    "    for k, v in zip(model.state_dict().keys(), final_ndarrays)\n",
    "}\n",
    "\n",
    "# Load the constructed state_dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Switch model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# Evaluate the loaded model on the held-out test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)  # Get predicted class indices\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "# Compute overall test accuracy\n",
    "accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "\n",
    "# Compute precision, recall, and F1 score (macro-averaged)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_targets, all_preds, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Define the path to save per-round and test metrics\n",
    "metrics_file = '/home/ir739wb/ilyarekun/bc_project/federated-learning/outputs/fed-avg-non-iid-metrics2.txt'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    # Write per-round validation metrics\n",
    "    rounds = range(1, len(strategy.metrics_history['val_loss']) + 1)\n",
    "    for round_num in rounds:\n",
    "        f.write(f\"Round {round_num}:\\n\")\n",
    "        f.write(f\"  Validation Loss: {strategy.metrics_history['val_loss'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy: {strategy.metrics_history['accuracy'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {strategy.metrics_history['precision'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {strategy.metrics_history['recall'][round_num-1]:.4f}\\n\")\n",
    "        f.write(f\"  F1 Score: {strategy.metrics_history['f1'][round_num-1]:.4f}\\n\")\n",
    "    \n",
    "    # Write overall test set metrics at the end of the file\n",
    "    f.write(\"\\nTest Metrics:\\n\")\n",
    "    f.write(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"  Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"  Recall: {recall:.4f}\\n\")\n",
    "    f.write(f\"  F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to '{metrics_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
